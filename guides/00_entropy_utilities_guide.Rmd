---
title: "[00] Information theory helpers Guide"
output:
  md_document:
    variant: gfm
output_dir: ../workshops
knitr:
  opts_knit:
    root.dir: ..
---

This tutorial complements `00_entropy_utilities.R` and unpacks the workshop on information theory helpers. You will see how it advances the Evaluation sequence while building confidence with base R and tidyverse tooling.

## Setup

- Ensure you have opened the `archr` project root (or set your working directory there) before running any code.
- Open the workshop script in RStudio so you can execute lines interactively with `Ctrl+Enter` or `Cmd+Enter`.
- Create a fresh R session to avoid conflicts with leftover objects from earlier workshops.

## Skills

- Navigate the script `00_entropy_utilities.R` within the Evaluation module.
- Connect the topic "Information theory helpers" to systems architecting decisions.
- Chain tidyverse verbs with `%>%` to explore stakeholder or architecture tables.
- Define custom functions to package repeatable logic.

## Process Overview









```mermaid
flowchart LR
    A[Define h()] --> B[Practice the Pipe]
    B[Practice the Pipe] --> C[Practice the Pipe (Step 11)]
    C[Practice the Pipe (Step 11)] --> D[Run the Code Block]
```

## Application

### Step 1 – Define `h()`

Create the helper function `h()` so you can reuse it throughout the workshop.

```{r step_01, eval=FALSE}
h = function(x){  -1*sum( x*log(x, base = 2) )  }
```

### Step 2 – Define `j()`

Create the helper function `j()` so you can reuse it throughout the workshop.

```{r step_02, eval=FALSE}
j = function(xy){ -1*sum(  sum( (xy) * log(xy, base = 2) ) ) }
```

### Step 3 – Define `i()`

Create the helper function `i()` so you can reuse it throughout the workshop.

```{r step_03, eval=FALSE}
i = function(x,y, xy){
  sum( sum( xy * log( ( xy / (x * y) ),  base = 2) ) )
}
```

### Step 4 – Define `ig()`

Create the helper function `ig()` so you can reuse it throughout the workshop.

```{r step_04, eval=FALSE}
ig = function(good, feature = NULL, .all = FALSE){
  # Testing values
  # feature = a$f
  # good = a$good
  data = data.frame(good)
  # Make a list container for the output
  output = list()
```

### Step 5 – Practice the Pipe

Tally up frequency of good vs. bad architectures.

```{r step_05, eval=FALSE}
  tally_good = data %>%
    group_by(good) %>%
    summarize(n = n(), .groups = "drop") %>%
    mutate(p = n / nrow(data))
```

### Step 6 – Practice the Pipe

Calculate entropy. Use the `%>%` operator to pass each result to the next tidyverse verb.

```{r step_06, eval=FALSE}
  stat = tally_good %>% 
    summarize(h = h(p))
```

### Step 7 – Run the Code Block

Add quantities to output.

```{r step_07, eval=FALSE}
  output$prob = tally_good
  output$stat = stat
```

### Step 8 – Run the Code Block

If feature is supplied...

```{r step_08, eval=FALSE}
  if(!is.null(feature)){
    data$feature = feature
```

### Step 9 – Practice the Pipe

Tally up frequency of good vs. bad given feature vs. not feature.

```{r step_09, eval=FALSE}
    tally_feature = data %>%
      group_by(good, feature) %>%
      # Count the architectures
      summarize(n = n(), .groups = "drop") %>%
      group_by(feature) %>%
      mutate(total = sum(n),
             p = n / total)
```

### Step 10 – Practice the Pipe

Get entropy by feature.

```{r step_10, eval=FALSE}
    h_by_feature = tally_feature %>%
      summarize(h = h(p),
                n = sum(n),
                fraction = n / nrow(data))
```

### Step 11 – Practice the Pipe

Get average weighted entropy.

```{r step_11, eval=FALSE}
    stat1 = h_by_feature %>%
      summarize(h_split = sum(fraction * h) )
```

### Step 12 – Practice the Pipe

Get the information gain.

```{r step_12, eval=FALSE}
    stat = bind_cols(stat, stat1) %>%
      mutate(ig = h - h_split)
```

### Step 13 – Run the Code Block

Add quantities to output.

```{r step_13, eval=FALSE}
    output$prob_split = tally_feature
    output$stat = stat
  }
```

### Step 14 – Run the Code Block

Execute the block and pay attention to the output it produces.

```{r step_14, eval=FALSE}
  if(.all == FALSE){
    output = output$stat
  }
```

### Step 15 – Run the Code Block

Execute the block and pay attention to the output it produces.

```{r step_15, eval=FALSE}
  return(output)
}
```

## Learning Checks











**Learning Check 1.** What role does the helper `h()` defined in Step 1 play in this workflow?

<details>
<summary>Show answer</summary>

It computes the Shannon entropy of a probability vector so you can quantify baseline uncertainty.

</details>

**Learning Check 2.** What role does the helper `j()` defined in Step 2 play in this workflow?

<details>
<summary>Show answer</summary>

It calculates the joint entropy across the `xy` distribution to feed later comparisons.

</details>

**Learning Check 3.** What role does the helper `i()` defined in Step 3 play in this workflow?

<details>
<summary>Show answer</summary>

It evaluates mutual information by contrasting the joint distribution with the product of marginals.

</details>

**Learning Check 4.** What role does the helper `ig()` defined in Step 4 play in this workflow?

<details>
<summary>Show answer</summary>

It orchestrates the information-gain workflow, collecting probabilities, entropy splits, and a tidy summary.

</details>
